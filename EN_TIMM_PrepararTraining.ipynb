{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EN_TIMM_PrepararTraining.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuxlR3lIOAMR6E83rl0Zay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manvgom/Pickioset/blob/main/EN_TIMM_PrepararTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53BuzybDEmgs",
        "outputId": "c7724232-f6b8-43a3-e24f-2dbb8cedba7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/MTDI_TFM/PICKIOSET/ends_timm\n"
          ]
        }
      ],
      "source": [
        "%reset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd drive/MyDrive/MTDI_TFM/PICKIOSET/ends_timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T-2HjGxExWr",
        "outputId": "0f9a6d41-131b-463d-a43e-bc5bd83d4645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 15.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import timm\n",
        "import timm.data\n",
        "import timm.loss\n",
        "import timm.optim\n",
        "import timm.utils\n",
        "import torch\n",
        "import torchmetrics\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "\n",
        "from pytorch_accelerated.callbacks import SaveBestModelCallback\n",
        "from pytorch_accelerated.trainer import Trainer, DEFAULT_CALLBACKS\n",
        "\n",
        "\n",
        "def create_datasets(image_size, data_mean, data_std, train_path, val_path):\n",
        "    train_transforms = timm.data.create_transform(\n",
        "        input_size=image_size,\n",
        "        is_training=True,\n",
        "        mean=data_mean,\n",
        "        std=data_std,\n",
        "        auto_augment=\"rand-m7-mstd0.5-inc1\",\n",
        "    )\n",
        "\n",
        "    eval_transforms = timm.data.create_transform(\n",
        "        input_size=image_size, mean=data_mean, std=data_std\n",
        "    )\n",
        "\n",
        "    train_dataset = timm.data.dataset.ImageDataset(\n",
        "        train_path, transform=train_transforms\n",
        "    )\n",
        "    eval_dataset = timm.data.dataset.ImageDataset(val_path, transform=eval_transforms)\n",
        "\n",
        "    return train_dataset, eval_dataset\n",
        "\n",
        "\n",
        "class TimmMixupTrainer(Trainer):\n",
        "    def __init__(self, eval_loss_fn, mixup_args, num_classes, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.eval_loss_fn = eval_loss_fn\n",
        "        self.num_updates = None\n",
        "        self.mixup_fn = timm.data.Mixup(**mixup_args)\n",
        "\n",
        "        self.accuracy = torchmetrics.Accuracy(num_classes=num_classes)\n",
        "        self.ema_accuracy = torchmetrics.Accuracy(num_classes=num_classes)\n",
        "        self.ema_model = None\n",
        "\n",
        "    def create_scheduler(self):\n",
        "        return timm.scheduler.CosineLRScheduler(\n",
        "            self.optimizer,\n",
        "            t_initial=self.run_config.num_epochs,\n",
        "            cycle_decay=0.5,\n",
        "            lr_min=1e-6,\n",
        "            t_in_epochs=True,\n",
        "            warmup_t=3,\n",
        "            warmup_lr_init=1e-4,\n",
        "            cycle_limit=1,\n",
        "        )\n",
        "\n",
        "    def training_run_start(self):\n",
        "        # Model EMA requires the model without a DDP wrapper and before sync batchnorm conversion\n",
        "        self.ema_model = timm.utils.ModelEmaV2(\n",
        "            self._accelerator.unwrap_model(self.model), decay=0.9\n",
        "        )\n",
        "        if self.run_config.is_distributed:\n",
        "            self.model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.model)\n",
        "\n",
        "    def train_epoch_start(self):\n",
        "        super().train_epoch_start()\n",
        "        self.num_updates = self.run_history.current_epoch * len(self._train_dataloader)\n",
        "\n",
        "    def calculate_train_batch_loss(self, batch):\n",
        "        xb, yb = batch\n",
        "        mixup_xb, mixup_yb = self.mixup_fn(xb, yb)\n",
        "        return super().calculate_train_batch_loss((mixup_xb, mixup_yb))\n",
        "\n",
        "    def train_epoch_end(\n",
        "        self,\n",
        "    ):\n",
        "        self.ema_model.update(self.model)\n",
        "        self.ema_model.eval()\n",
        "\n",
        "        if hasattr(self.optimizer, \"sync_lookahead\"):\n",
        "            self.optimizer.sync_lookahead()\n",
        "\n",
        "    def scheduler_step(self):\n",
        "        self.num_updates += 1\n",
        "        if self.scheduler is not None:\n",
        "            self.scheduler.step_update(num_updates=self.num_updates)\n",
        "\n",
        "    def calculate_eval_batch_loss(self, batch):\n",
        "        with torch.no_grad():\n",
        "            xb, yb = batch\n",
        "            outputs = self.model(xb)\n",
        "            val_loss = self.eval_loss_fn(outputs, yb)\n",
        "            self.accuracy.update(outputs.argmax(-1), yb)\n",
        "\n",
        "            ema_model_preds = self.ema_model.module(xb).argmax(-1)\n",
        "            self.ema_accuracy.update(ema_model_preds, yb)\n",
        "\n",
        "        return {\"loss\": val_loss, \"model_outputs\": outputs, \"batch_size\": xb.size(0)}\n",
        "\n",
        "    def eval_epoch_end(self):\n",
        "        super().eval_epoch_end()\n",
        "\n",
        "        if self.scheduler is not None:\n",
        "            self.scheduler.step(self.run_history.current_epoch + 1)\n",
        "\n",
        "        self.run_history.update_metric(\"accuracy\", self.accuracy.compute().cpu())\n",
        "        self.run_history.update_metric(\n",
        "            \"ema_model_accuracy\", self.ema_accuracy.compute().cpu()\n",
        "        )\n",
        "        self.accuracy.reset()\n",
        "        self.ema_accuracy.reset()\n",
        "\n",
        "\n",
        "def main(data_path):\n",
        "\n",
        "    # Set training arguments, hardcoded here for clarity\n",
        "    image_size = (224, 224)\n",
        "    lr = 5e-3\n",
        "    smoothing = 0.1\n",
        "    mixup = 0.2\n",
        "    cutmix = 1.0\n",
        "    batch_size = 32\n",
        "    bce_target_thresh = 0.2\n",
        "    num_epochs = 40\n",
        "\n",
        "    data_path = Path(data_path)\n",
        "    train_path = data_path / \"train\"\n",
        "    val_path = data_path / \"val\"\n",
        "    num_classes = len(list(train_path.iterdir()))\n",
        "\n",
        "    mixup_args = dict(\n",
        "        mixup_alpha=mixup,\n",
        "        cutmix_alpha=cutmix,\n",
        "        label_smoothing=smoothing,\n",
        "        num_classes=num_classes,\n",
        "    )\n",
        "\n",
        "    # Create model using timm\n",
        "    model = timm.create_model(\n",
        "        \"efficientnet_b0\", pretrained=False, num_classes=num_classes, drop_path_rate=0.05\n",
        "    )\n",
        "\n",
        "    # Load data config associated with the model to use in data augmentation pipeline\n",
        "    data_config = timm.data.resolve_data_config({}, model=model, verbose=True)\n",
        "    data_mean = data_config[\"mean\"]\n",
        "    data_std = data_config[\"std\"]\n",
        "\n",
        "    # Create training and validation datasets\n",
        "    train_dataset, eval_dataset = create_datasets(\n",
        "        train_path=train_path,\n",
        "        val_path=val_path,\n",
        "        image_size=image_size,\n",
        "        data_mean=data_mean,\n",
        "        data_std=data_std,\n",
        "    )\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = timm.optim.create_optimizer_v2(\n",
        "        model, opt=\"lookahead_AdamW\", lr=lr, weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    # As we are using Mixup, we can use BCE during training and CE for evaluation\n",
        "    train_loss_fn = timm.loss.BinaryCrossEntropy(\n",
        "        target_threshold=bce_target_thresh, smoothing=smoothing\n",
        "    )\n",
        "    validate_loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Create trainer and start training\n",
        "    trainer = TimmMixupTrainer(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        loss_func=train_loss_fn,\n",
        "        eval_loss_fn=validate_loss_fn,\n",
        "        mixup_args=mixup_args,\n",
        "        num_classes=num_classes,\n",
        "        callbacks=[\n",
        "            *DEFAULT_CALLBACKS,\n",
        "            SaveBestModelCallback(watch_metric=\"accuracy\", greater_is_better=True),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    trainer.train(\n",
        "        per_device_batch_size=batch_size,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        num_epochs=num_epochs,\n",
        "        create_scheduler_fn=trainer.create_scheduler,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Simple example of training script using timm.\")\n",
        "    parser.add_argument(\"--data_dir\", required=True, help=\"The data folder on disk.\")\n",
        "    args = parser.parse_args()\n",
        "    main(args.data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RYm4MwNE3w9",
        "outputId": "3533b0b3-4206-444c-dbf7-e19fe4ed8466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    }
  ]
}